# Train parameters
model: distilgpt2
pretrained: True
tokenizer: gpt2
gpu: 0
optimizer: AdamW
scheduler: LinearLambdaLR
batch-size: 32
max-length: 512
max-epochs: 250
num-workers: 4
num-trials: 1
loss: cross_entropy
clip-grad: 1.0
optimizer-kwargs:
  lr: 0.00005
scheduler-kwargs:
  num_warmup_steps: 0
mask: False

# Data Parameters
name: multiwoz2.1
train-src: _test-dataset/train.txt
# train-tgt: /home/mat/github/U-of-T/grad-school/nlp/tod/simpletod/resources/gpt2/train.history_belief
val-src: _test-dataset/val.txt
# val-tgt: /home/mat/github/U-of-T/grad-school/nlp/tod/simpletod/resources/gpt2/val.history_belief
task: causal
overwrite_cache: True

# io
output: _test-output
checkpoint: _test-checkpoint
cache-dir: _test-cache
save-freq: 1
