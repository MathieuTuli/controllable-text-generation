# Train parameters
model: distilgpt2
tokenizer: gpt2
pretrained: False
optimizer: AdamW
scheduler: LinearLambdaLR
batch-size: 1
max-length: 512
max-epochs: 10
num-trials: 2
loss: cross_entropy
clip-grad: 1.0
optimizer-kwargs:
  lr: 0.00005
scheduler-kwargs:
  num_warmup_steps: 0
mask: False

# Data Parameters
name: multiwoz2.1
train-src: _test-dataset/train.txt
# train-tgt: /home/mat/github/U-of-T/grad-school/nlp/tod/simpletod/resources/gpt2/train.history_belief
val-src: _test-dataset/val.txt
# val-tgt: /home/mat/github/U-of-T/grad-school/nlp/tod/simpletod/resources/gpt2/val.history_belief
task: clm
overwrite_cache: True
num-workers: 4

# io
output: _test-output
checkpoint: _test-checkpoint
cache-dir: _test-cache
save-freq: 1
