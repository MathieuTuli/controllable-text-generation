# Train parameters
model: facebook/wmt19-en-ru
tokenizer: facebook/wmt19-en-ru
model-pretrained: False
tokenizer-pretrained: False
optimizer: AdamW
scheduler: LinearLambdaLR
batch-size: 4
max-src-length: 512
max-tgt-length: 512
ignore-pad-for-loss: True
pad-to-max-length: True
max-epochs: 100
num-trials: 1
loss: cross_entropy
clip-grad: 1.0
optimizer-kwargs:
  learning_rate: 0.0005
  weight_decay: 0.0
scheduler-kwargs:
  warmup_ratio: 0.
  warmup_steps: 1500
mask: False

# Data Parameters
name: smcalflow
train-src: /home/mat/archive/datasets/smcalflow/prepared-context2/train.json
val-src: /home/mat/archive/datasets/smcalflow/prepared-context2/valid.json
vocab: /home/mat/archive/datasets/smcalflow/prepared-context2/vocab.txt
task: nmt
overwrite_cache: True
num-workers: 4
max-train-samples: 5000
max-val-samples: 5000
tokenizer-files:
  - /home/mat/archive/datasets/smcalflow/prepared-context2/train.src
  - /home/mat/archive/datasets/smcalflow/prepared-context2/train.tgt
  - /home/mat/archive/datasets/smcalflow/prepared-context2/valid.src
  - /home/mat/archive/datasets/smcalflow/prepared-context2/valid.tgt

# io
output: /home/mat/archive/playground/_test-output
checkpoint: /home/mat/archive/playground/_test-checkpoint
cache-dir: /home/mat/archive/playground/_test-cache
save-freq: 1
